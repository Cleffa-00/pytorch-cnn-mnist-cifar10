# ğŸ–¼ï¸ CNN Image Classification with PyTorch

A clean, modular PyTorch implementation of Convolutional Neural Networks for image classification on CIFAR-10 and MNIST datasets.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## ğŸ“Š Results

| Dataset   | Model       | Parameters | Test Accuracy | Training Time |
|-----------|-------------|------------|---------------|---------------|
| CIFAR-10  | CIFAR10Net  | 288K       | **90.49%**    | ~13 min (GPU) |
| MNIST     | MNISTNet    | 241K       | **99.70%**    | ~6 min (GPU)  |

### Sample Predictions

![CIFAR-10 Demo](assets/cifar10_demo.png)

![MNIST Demo](assets/mnist_demo.png)

### Per-Class Accuracy

<details>
<summary>CIFAR-10 Classes</summary>

| Class  | Accuracy |
|--------|----------|
| Plane  | 90%+     |
| Car    | 95%+     |
| Bird   | 85%+     |
| Cat    | 80%+     |
| Deer   | 88%+     |
| Dog    | 82%+     |
| Frog   | 95%+     |
| Horse  | 90%+     |
| Ship   | 95%+     |
| Truck  | 94%+     |

</details>

<details>
<summary>MNIST Digits</summary>

| Digit | Accuracy |
|-------|----------|
| 0     | 99.9%    |
| 1     | 100.0%   |
| 2     | 99.8%    |
| 3     | 100.0%   |
| 4     | 99.6%    |
| 5     | 99.6%    |
| 6     | 99.5%    |
| 7     | 99.5%    |
| 8     | 99.6%    |
| 9     | 99.5%    |

</details>

## ğŸ—ï¸ Architecture

### CIFAR-10 CNN (6 Conv Layers)
```
Input (3Ã—32Ã—32)
    â†“
[Conv3Ã—3 â†’ BN â†’ ReLU] Ã— 2 â†’ MaxPool  (32 filters)
    â†“
[Conv3Ã—3 â†’ BN â†’ ReLU] Ã— 2 â†’ MaxPool  (64 filters)
    â†“
[Conv3Ã—3 â†’ BN â†’ ReLU] Ã— 2 â†’ MaxPool  (128 filters)
    â†“
Global Average Pooling â†’ FC(128â†’10)
    â†“
Output (10 classes)
```

**Key Features:**
- âœ… Batch Normalization for stable training
- âœ… Global Average Pooling (reduces parameters & overfitting)
- âœ… Data Augmentation (RandomAffine, HorizontalFlip)
- âœ… Cosine Annealing LR Schedule
- âœ… GPU Acceleration (CUDA)

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/Cleffa-00/pytorch-cnn-mnist-cifar10.git
cd pytorch-cnn-mnist-cifar10
pip install -r requirements.txt
```

### Training

**CIFAR-10** (50 epochs, ~13 min on GPU):
```bash
python train_cifar.py --epochs 50
```

**MNIST** (30 epochs, ~6 min on GPU):
```bash
python train_mnist.py --epochs 30
```

### Custom Options
```bash
python train_cifar.py --epochs 100 --batch-size 256 --lr 0.1 --data-dir ./data
```

### Quick Inference Demo

Run predictions without training (uses pre-trained weights):
```bash
python predict.py
```

Output:
```
ğŸš€ PyTorch CNN Inference Demo

==================================================
ğŸ–¼ï¸  CIFAR-10 Inference Demo
==================================================
âœ… Model loaded on cuda

ğŸ“Š Predictions on random test images:

  âœ“ Predicted: Car ğŸš—       | Actual: Car ğŸš—
  âœ“ Predicted: Ship ğŸš¢      | Actual: Ship ğŸš¢
  âœ“ Predicted: Frog ğŸ¸      | Actual: Frog ğŸ¸
  ...
```

## ğŸ“ Project Structure
```
pytorch-cnn-mnist-cifar10/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ cnn.py              # CNN architectures
â”œâ”€â”€ checkpoints/            # Saved models (.pth) - auto-created on training
â”œâ”€â”€ data/                   # Downloaded datasets - auto-created on first run
â”œâ”€â”€ train_cifar.py          # CIFAR-10 training
â”œâ”€â”€ train_mnist.py          # MNIST training
â”œâ”€â”€ predict.py              # Quick inference demo
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ”§ Training Configuration

| Parameter        | CIFAR-10 | MNIST  |
|------------------|----------|--------|
| Epochs           | 50       | 30     |
| Batch Size       | 128      | 128    |
| Learning Rate    | 0.1      | 0.1    |
| Optimizer        | SGD      | SGD    |
| Momentum         | 0.9      | 0.9    |
| Weight Decay     | 5e-4     | 5e-4   |
| LR Schedule      | Cosine Annealing | Cosine Annealing |

### Data Augmentation

**CIFAR-10:**
- Random Affine (rotation Â±15Â°, translation 10%, scale 0.9-1.1)
- Random Horizontal Flip
- Normalization (mean=0.5, std=0.5)

**MNIST:**
- Random Affine (rotation Â±15Â°, translation 10%, scale 0.9-1.1)
- Normalization (mean=0.5, std=0.5)

## ğŸ“ˆ Training Features

- ğŸ“Š Real-time progress bars with tqdm
- ğŸ’¾ Automatic best model checkpointing
- ğŸ“‰ Per-epoch train/test metrics
- ğŸ¯ Per-class accuracy evaluation
- âš¡ GPU acceleration support

## ğŸ”¬ Model Loading

```python
import torch
from models.cnn import CIFAR10Net, MNISTNet

# Load CIFAR-10 model
cifar_model = CIFAR10Net()
cifar_model.load_state_dict(torch.load('checkpoints/cifar10_best.pth', weights_only=True))
cifar_model.eval()

# Load MNIST model
mnist_model = MNISTNet()
mnist_model.load_state_dict(torch.load('checkpoints/mnist_best.pth', weights_only=True))
mnist_model.eval()
```

## ğŸ› ï¸ Requirements

- Python 3.10+
- PyTorch 2.0+
- torchvision
- tqdm
- CUDA (optional, for GPU acceleration)

## ğŸ“ License

MIT License

## ğŸ™ Acknowledgments

- PyTorch team for the deep learning framework
- CIFAR-10 and MNIST dataset creators
